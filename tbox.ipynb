{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community neo4j openai wikipedia tiktoken langchain_openai pdfplumber python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have overwritten the properties value to be a list of Property classes instead of a dictionary to overcome the limitations of the API. Because you can only pass a single object to the API, we can to combine the nodes and relationships in a single class called KnowledgeGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "from langchain.schema import (\n",
    "   AIMessage,\n",
    "   HumanMessage,\n",
    "   SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(\"GPT-4o\")\n",
    "\n",
    "input = f\"\"\"This directive defines the roles and responsibilities for managing and overseeing NASA’s nuclear\n",
    "flight safety activities. It provides the requirements to implement NASA’s policy to protect the\n",
    "public, NASA workforce, high-value equipment and property, and the environment from potential\n",
    "harm as a result of NASA activities and operations, by factoring safety as an integral feature of\n",
    "programs, projects, technologies, operations, and facilities.\n",
    "b. This directive also describes NASA’s implementation of Federal requirements under National\n",
    "Security Presidential Memorandum (NSPM)-20, “Presidential Memorandum on Launch of\n",
    "Spacecraft Containing Space Nuclear Systems,” dated August 20, 2019, radiological contingency\n",
    "planning (RCP) as a part of broader NASA emergency management activities (see NPD 8710.1 and\n",
    "NPR 8715.2) and other factors, as well as agency-specific activities relating to ensuring safety and\n",
    "mission success for NASA-sponsored payloads containing space nuclear systems (SNS) or other\n",
    "radioactive material (note that these terms are defined in Appendix A).\n",
    "c. This directive establishes a framework where other requirements, guidance, and processes (e.g.,\n",
    "Department of Energy (DOE) nuclear safety and security requirements, U.S. Air and Space Force\n",
    "range safety requirements, NASA payload safety processes) relevant to nuclear flight safety can be\n",
    "implemented in to the overall Safety and Mission Assurance (SMA) process.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "Step 1:\n",
    "Split each sentence from the text into a set of entailed clauses that are maximally shortened. Format the clauses into RDF triples that have only two commas and show them only. No explanation needed. \n",
    "\n",
    "For instance, the below sentence:\n",
    "This directive defines the roles and responsibilities for managing and overseeing NASA’s nuclear flight safety activities. Lions, zebras, and whales are animals.\n",
    "\n",
    "Should be split like so:\n",
    "This directive, defines, the roles and responsibilities\n",
    "The roles and responsibilities, are for, managing and overseeing NASA’s nuclear flight safety activities\n",
    "Lions, are, animals\n",
    "zebras, are, animals\n",
    "whales, are, animals\n",
    "\n",
    "Step 2: \n",
    "Treat the triples as an A-box ontology and generate a corresponding OWL2-DL T-box ontology in turtle format. Derive general names for classes of subjects and objects (avoid using \n",
    "individual names from the triples). However, use predicate names as property names without change. Make sure all classes are used and are related as either domains of ranges of object properties.\n",
    "\n",
    "Step 3: \n",
    "Parse the triples from step 1 into a readable A-Box ontology in turtle format using the terms of the above T-box. Group the triples by subject. Use words from the text \n",
    "directly as individual names..\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "   SystemMessage(content=input),\n",
    "   HumanMessage(content=prompt)\n",
    "]\n",
    "response = llm(messages)\n",
    "print(\"LLM Response: \\n\" + response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the general instructions, I have also added the option to limit which node or relationship types should be extracted from text. You'll see through examples why this might come in handy. We have the Neo4j connection and LLM prompt ready, which means we can define the information extraction pipeline as a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    print(data)\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in a LangChain document as well as optional nodes and relationship parameters, which are used to limit the types of objects we want the LLM to identify and extract. A month or so ago, we added the add_graph_documents method the Neo4j graph object, which we can utilize here to seamlessly import the graph.\n",
    "\n",
    "# Evaluation\n",
    "We will extract information from the Tom Hanks Wikipedia page and construct a knowledge graph to test the pipeline. Here, we will utilize the Wikipedia loader and text chunking modules provided by LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that we use a relatively large chunk_size value. The reason is that we want to provide as much context as possible around a single sentence in order for the coreference resolution part to work as best as possible. Remember, the coreference step will only work if the entity and its reference appear in the same chunk; otherwise, the LLM doesn't have enough information to link the two.\n",
    "\n",
    "Now we can go ahead and run the documents through the information extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "import pdfplumber\n",
    "from html import escape\n",
    "\n",
    "def pdf_to_html(pdf_path, html_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        html = '<html><body>'\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # Extract text\n",
    "            page_text = page.extract_text()\n",
    "            html += f'<p>{page_text}</p>'\n",
    "            \n",
    "            # Extract tables\n",
    "            for table in page.extract_tables():\n",
    "                html += '<table>'\n",
    "                for row in table:\n",
    "                    html += '<tr>'\n",
    "                    for cell in row:\n",
    "                        html += f'<td>{escape(cell)}</td>' if cell is not None else '<td></td>'\n",
    "                        # escape html tags in text\n",
    "                        \n",
    "                    html += '</tr>'\n",
    "                html += '</table>'\n",
    "        html += '</body></html>'\n",
    "    \n",
    "    # Write HTML to file\n",
    "    with open(html_path, 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(html)\n",
    "    return html\n",
    "\n",
    "text_content = pdf_to_html('big sample.pdf', 'output.html')\n",
    "\n",
    "# Create a Document object\n",
    "document = Document(text_content)\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "\n",
    "# Split the document into chunks\n",
    "documents = text_splitter.split_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "'''client = OpenAI(\n",
    "      api_key = os.environ.get(\"\")\n",
    "   )\n",
    "\n",
    "with open(\"output.html\", \"r\") as file:\n",
    "   context = file.read()\n",
    "   \n",
    "messages =[\n",
    "         {\"role\": \"system\", \"content\": context},\n",
    "         {\"role\": \"user\", \"content\": \"Generate JSON scheme\"},\n",
    "      ]\n",
    "   \n",
    "# Make API request        \n",
    "completion = client.chat.completions.create(\n",
    "   model=\"gpt-4-turbo-preview\",\n",
    "   messages=messages,\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content\n",
    "print(output)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    extract_and_store_graph(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process takes around 5 minutes, which is relatively slow. Therefore, you would probably want parallel API calls in production to deal with this problem and achieve some sort of scalability. Let's first look at the types of nodes and relationships the LLM identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the graph\n",
    "#graph.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which node labels should be extracted by the LLM\n",
    "#allowed_nodes = [\"Person\", \"Company\", \"Location\", \"Event\", \"Movie\", \"Service\", \"Award\"]\n",
    "\n",
    "#for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "#    extract_and_store_graph(d, allowed_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Application\n",
    "The last thing we will do is show you how you can browse information in a knowledge graph by constructing Cypher statements. Cypher is a structured query language used to work with graph databases, similar to how SQL is used for relational databases. LangChain has a GraphCypherQAChain that reads the schema of the graph and constructs appropriate Cypher statements based on the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the content of Chief, Safety And Mission Assurance?\"\n",
    "cypher_chain.invoke({\"query\": query})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
